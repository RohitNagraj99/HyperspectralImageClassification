# -*- coding: utf-8 -*-
"""IndianPines_RMDL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Emem8ob60fX1uyBvQdKAIhJIueDYWH7L

## **Hyperspectral image classification on Indian Pines dataset**

This notebook implements the HybridSN architecture to achieve the state of the art on Indian pines dataset with a test accuracy of 98.80%
"""

# %tensorflow_version 2.1
from google.colab import drive
drive.mount('/content/drive')
! pip install spectral
! pip install RMDL;

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization
from tensorflow.keras.layers import Dropout, Input, LeakyReLU, AveragePooling3D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

from RMDL import RMDL_Image as RMDL

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score

from plotly.offline import init_notebook_mode

from operator import truediv

import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import os
import spectral

init_notebook_mode(connected=True)
# %matplotlib inline

print(tf.__version__)

"""#### Data loading"""

# Global Variables
dataset = 'IP'
test_ratio = 0.5
windowSize = 25

def loadData():
    data_path = os.path.join(os.getcwd(), 'Dataset')
    data = sio.loadmat('/content/drive/My Drive/Colab Notebooks/Datasets/Indian_Pines/Indian_pines_corrected.mat')['indian_pines_corrected']
    labels = sio.loadmat('/content/drive/My Drive/Colab Notebooks/Datasets/Indian_Pines/Indian_pines_gt.mat')['indian_pines_gt']
    return data, labels

def splitTrainTestSet(X, y, testRatio, randomState=0):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testRatio,
                                                       random_state=randomState,
                                                       stratify=y)
    return X_train, X_test, y_train, y_test
"""
This stratify parameter makes a split so that the proportion of values in the sample 
produced will be the same as the proportion of values provided to parameter stratify.

For example, if variable y is a binary categorical variable with values 0 and 1 and 
there are 25% of zeros and 75% of ones, stratify=y will make sure that your random 
split has 25% of 0's and 75% of 1's.
""";

def applyPCA(X, numComponents=75):
    newX = np.reshape(X, (-1, X.shape[2]))
    pca = PCA(n_components=numComponents, whiten=True)
    newX = pca.fit_transform(newX)
    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))
    return newX, pca
"""
When True (False by default) the components_ vectors are multiplied by the square 
root of n_samples and then divided by the singular values to ensure uncorrelated 
outputs with unit component-wise variances.

Whitening will remove some information from the transformed signal (the relative 
variance scales of the components) but can sometime improve the predictive accuracy 
of the downstream estimators by making their data respect some hard-wired assumptions.
""";

def padWithZeros(X, margin=2):
    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2*margin, X.shape[2]))
    x_offset = margin
    y_offset = margin
    newX[x_offset:X.shape[0] + x_offset, y_offset: X.shape[1] + y_offset, :] = X
    return newX

def createImageCubes(X, y, windowSize=5, removeZeroLabels=True):
    margin = int((windowSize-1)/2)
    zeroPaddedX = padWithZeros(X, margin=margin)
    
    # Split patches
    print("X.shape", X.shape)
    patchesData = np.zeros((X.shape[0]*X.shape[1], windowSize, windowSize, X.shape[2]))
    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
    patchIndex = 0
    
    for r in range(margin, zeroPaddedX.shape[0] - margin):
        for c in range(margin, zeroPaddedX.shape[1] - margin):
            patch = zeroPaddedX[r - margin: r + margin + 1, c - margin:c + margin + 1]
            patchesData[patchIndex, :, :, :] = patch
            patchesLabels[patchIndex] = y[r-margin, c-margin]
            patchIndex += 1
            
    if removeZeroLabels:
        patchesData = patchesData[patchesLabels > 0, :, :, :]
        patchesLabels = patchesLabels[patchesLabels > 0]
        patchesLabels -= 1
    
    return patchesData, patchesLabels

X, y = loadData()

X.shape, y.shape

K = X.shape[2] # Number of bands

# We'll apply PCA to get only 30 components
K = 30
X, pca = applyPCA(X, numComponents = K)
X.shape

X, y = createImageCubes(X, y, windowSize = windowSize)
X.shape, y.shape
X = X.astype('float32')
y = y.astype('uint8')
# Its picking 25x25 grid size, which i feel is a little too large.

# Inference: Thre are a total of 145x145 = 21,025 pixels, and none is lost
# during creation of patches due to padding.
# We lose (21025 - 10249 = 10,776) pixels as they were 0, meaning unclassified.

X = np.interp(X, (X.min(), X.max()), (0, +1))
X_train, X_test, y_train, y_test = splitTrainTestSet(X, y, test_ratio)
print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("y_train shape: ", y_train.shape)
print("y_test shape: ", y_test.shape)
# Not sure why the train test split is so large

# y_train = to_categorical(y_train)  # One hot encoding basically
y_train.shape
# y_test = to_categorical(y_test)
y_test.shape

X_test.shape

X_train = X_train.reshape(-1, windowSize, windowSize, K)
X_train.shape
X_test = X_test.reshape(-1, windowSize, windowSize, K)
X_test.shape

num_of_classes = 16

shape = (25, 25, K)
batch_size = 64

sparse_categorical = True
n_epochs = [100,100,100]
random_deep = [3,3,3]

y_train.dtype

RMDL.Image_Classification(X_train, y_train, X_test, y_test,shape,
                     batch_size=batch_size,
                     sparse_categorical=True,
                     random_deep=random_deep,
                     epochs=n_epochs)

# Inference: Only cnn worked well, all others failed. Hence, it didnt make any difference