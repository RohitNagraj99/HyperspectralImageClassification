{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "IndianPines_Keras_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3cx4kLkXHZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8fd4d4d2-bf47-4d6f-9673-b75b3f57d978"
      },
      "source": [
        "%tensorflow_version 2.1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! pip install spectral"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.1`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting spectral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/8e/db1d750fb0153027e4e945f91f04b72a3b8b9a0cfdc2c8a33bedcb27740d/spectral-0.20.tar.gz (143kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spectral) (1.18.1)\n",
            "Building wheels for collected packages: spectral\n",
            "  Building wheel for spectral (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spectral: filename=spectral-0.20-cp36-none-any.whl size=183917 sha256=7e747baf04612311de6020701033063819792bf6351faed417922bed6ee5e1e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/cf/f3/3cab28f6aed46f15c8db09c6ad678483610426261025e61ff8\n",
            "Successfully built spectral\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdWa5s22WiE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "60ed164c-4909-4f2c-cc95-c81547021342"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout, Input, LeakyReLU, AveragePooling3D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "\n",
        "from plotly.offline import init_notebook_mode\n",
        "\n",
        "from operator import truediv\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ],
      "execution_count": 490,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBxCiRfuedcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56943167-bd7c-43cf-c7be-fdc6a77c2976"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orRjgxecWiFJ",
        "colab_type": "text"
      },
      "source": [
        "#### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZXmFqpKWiFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global Variables\n",
        "dataset = 'IP'\n",
        "test_ratio = 0.6\n",
        "val_ratio = 0.5\n",
        "windowSize = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L8D-5C4WiFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData():\n",
        "    data_path = os.path.join(os.getcwd(), 'Dataset')\n",
        "    data = sio.loadmat('/content/drive/My Drive/Colab Notebooks/Datasets/Indian_Pines/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "    labels = sio.loadmat('/content/drive/My Drive/Colab Notebooks/Datasets/Indian_Pines/Indian_pines_gt.mat')['indian_pines_gt']\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTDH4YxWiFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=0):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testRatio,\n",
        "                                                       random_state=randomState,\n",
        "                                                       stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\"\"\"\n",
        "This stratify parameter makes a split so that the proportion of values in the sample \n",
        "produced will be the same as the proportion of values provided to parameter stratify.\n",
        "\n",
        "For example, if variable y is a binary categorical variable with values 0 and 1 and \n",
        "there are 25% of zeros and 75% of ones, stratify=y will make sure that your random \n",
        "split has 25% of 0's and 75% of 1's.\n",
        "\"\"\";"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6btjMa1WiFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
        "    return newX, pca\n",
        "\"\"\"\n",
        "When True (False by default) the components_ vectors are multiplied by the square \n",
        "root of n_samples and then divided by the singular values to ensure uncorrelated \n",
        "outputs with unit component-wise variances.\n",
        "\n",
        "Whitening will remove some information from the transformed signal (the relative \n",
        "variance scales of the components) but can sometime improve the predictive accuracy \n",
        "of the downstream estimators by making their data respect some hard-wired assumptions.\n",
        "\"\"\";"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy9UwrrdWiFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2*margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset: X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1accf5oWiFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels=True):\n",
        "    margin = int((windowSize-1)/2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    \n",
        "    # Split patches\n",
        "    print(\"X.shape\", X.shape)\n",
        "    patchesData = np.zeros((X.shape[0]*X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    \n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin: r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex += 1\n",
        "            \n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels > 0, :, :, :]\n",
        "        patchesLabels = patchesLabels[patchesLabels > 0]\n",
        "        patchesLabels -= 1\n",
        "    \n",
        "    return patchesData, patchesLabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TV5hjOdWiFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "351289cb-bb6d-44ea-cba8-802ef1f34740"
      },
      "source": [
        "X, y = loadData()\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((145, 145, 200), (145, 145))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 466
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG2H-LrHWiF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = X.shape[2] # Number of bands"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS8XUN5WiF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33a64259-ce74-43a9-8211-f5844dc596cb"
      },
      "source": [
        "# We'll apply PCA to get only 30 components\n",
        "K = 75\n",
        "X, pca = applyPCA(X, numComponents = K)\n",
        "X.shape"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145, 145, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 468
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGyxdbh9WiGC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8247bd0f-afc7-42f1-d479-23350d2b01e7"
      },
      "source": [
        "X, y = createImageCubes(X, y, windowSize = windowSize)\n",
        "X.shape, y.shape\n",
        "# Its picking 25x25 grid size, which i feel is a little too large."
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape (145, 145, 75)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10249, 15, 15, 75), (10249,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 469
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AJH5l75WiGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference: Thre are a total of 145x145 = 21,025 pixels, and none is lost\n",
        "# during creation of patches due to padding.\n",
        "# We lose (21025 - 10249 = 10,776) pixels as they were 0, meaning unclassified."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDvR_bAfWiGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "691f733c-dcf3-4f16-eaa7-29b2acbc9790"
      },
      "source": [
        "X_train, X_test, y_train, y_test = splitTrainTestSet(X, y, test_ratio)\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"y_test shape: \", y_test.shape)\n",
        "# Not sure why the train test split is so large"
      ],
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (4099, 15, 15, 75)\n",
            "X_test shape:  (6150, 15, 15, 75)\n",
            "y_train shape:  (4099,)\n",
            "y_test shape:  (6150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNBvrKVkQu3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val, X_test, y_val, y_test = splitTrainTestSet(X_test, y_test, val_ratio)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYm3j8F_WiGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e62c9b33-891c-4f77-c2f7-7d56a35fec7d"
      },
      "source": [
        "y_train = to_categorical(y_train)  # One hot encoding basically\n",
        "y_train.shape\n",
        "y_test = to_categorical(y_test)\n",
        "y_test.shape\n",
        "y_val = to_categorical(y_val)\n",
        "y_val.shape"
      ],
      "execution_count": 473,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3075, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJN0J0u3Rjsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a74ba12c-3f1d-4b4f-ff7d-11cdb292e008"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3075, 15, 15, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttrD-2KgWiGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "51d5a162-213b-48b8-b428-55656413152e"
      },
      "source": [
        "X_train = X_train.reshape(-1, windowSize, windowSize, K, 1)\n",
        "X_train.shape\n",
        "X_test = X_test.reshape(-1, windowSize, windowSize, K, 1)\n",
        "X_test.shape\n",
        "X_val = X_val.reshape(-1, windowSize, windowSize, K, 1)\n",
        "X_val.shape"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3075, 15, 15, 75, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 475
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj-VC9PxWiGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab86d824-03f7-4506-a4a2-650b8877ad4e"
      },
      "source": [
        "# Model parameters\n",
        "S = windowSize\n",
        "L = K\n",
        "output_units = 16\n",
        "print(S, L, output_units)"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15 75 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7E8z395WiGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88442a7a-5f54-43ae-87ad-d14deb156646"
      },
      "source": [
        "input_layer = Input((S, S, L, 1))\n",
        "\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3,3,7), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=8, kernel_size=(3,3,5), activation='relu')(conv_layer1)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=16, kernel_size=(3,3,3), activation='relu')(conv_layer2)\n",
        "print(conv_layer3.shape)\n",
        "\n",
        "conv3d_shape = conv_layer3.shape\n",
        "\n",
        "conv_layer3 = Reshape((conv3d_shape[1], conv3d_shape[2],\n",
        "                    conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
        "\n",
        "conv_layer4 = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(conv_layer3)\n",
        "\n",
        "flatten_layer = Flatten()(conv_layer4)\n",
        "\n",
        "dense_layer1 = Dense(units = 256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer1)\n",
        "\n",
        "output_layer = Dense(units=output_units, activation='softmax')(dense_layer2)"
      ],
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 9, 9, 63, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EIOjlRWiGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "bbb01825-ee34-4138-971b-c49010a24b50"
      },
      "source": [
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        [(None, 15, 15, 75, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_64 (Conv3D)           (None, 13, 13, 69, 8)     512       \n",
            "_________________________________________________________________\n",
            "conv3d_65 (Conv3D)           (None, 11, 11, 65, 8)     2888      \n",
            "_________________________________________________________________\n",
            "conv3d_66 (Conv3D)           (None, 9, 9, 63, 16)      3472      \n",
            "_________________________________________________________________\n",
            "reshape_20 (Reshape)         (None, 9, 9, 1008)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 7, 7, 64)          580672    \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 256)               803072    \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 16)                4112      \n",
            "=================================================================\n",
            "Total params: 1,394,728\n",
            "Trainable params: 1,394,728\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grx_rEdVWiGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = Adam(lr = 0.001, decay = 1e-06)\n",
        "adamax = Adamax()\n",
        "model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvdE5rHXWiGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint\n",
        "filepath = 'best-model.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsk_4VVpWiG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edbe86db-e402-4f73-83a4-8ecddfe86ffa"
      },
      "source": [
        "history = model.fit(x=X_train, y=y_train, batch_size=64, epochs=100, callbacks=callbacks_list, validation_data=(X_val, y_val))"
      ],
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4099 samples, validate on 3075 samples\n",
            "Epoch 1/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 1.6143 - accuracy: 0.4839\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89593, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 3s 679us/sample - loss: 1.5768 - accuracy: 0.4965 - val_loss: 0.3567 - val_accuracy: 0.8959\n",
            "Epoch 2/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9272\n",
            "Epoch 00002: val_accuracy improved from 0.89593 to 0.97073, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 528us/sample - loss: 0.2536 - accuracy: 0.9273 - val_loss: 0.0868 - val_accuracy: 0.9707\n",
            "Epoch 3/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9725\n",
            "Epoch 00003: val_accuracy improved from 0.97073 to 0.98081, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 530us/sample - loss: 0.0959 - accuracy: 0.9724 - val_loss: 0.0541 - val_accuracy: 0.9808\n",
            "Epoch 4/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9831\n",
            "Epoch 00004: val_accuracy improved from 0.98081 to 0.98309, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 522us/sample - loss: 0.0681 - accuracy: 0.9827 - val_loss: 0.0507 - val_accuracy: 0.9831\n",
            "Epoch 5/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9831\n",
            "Epoch 00005: val_accuracy improved from 0.98309 to 0.98959, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 532us/sample - loss: 0.0594 - accuracy: 0.9834 - val_loss: 0.0335 - val_accuracy: 0.9896\n",
            "Epoch 6/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9878\n",
            "Epoch 00006: val_accuracy improved from 0.98959 to 0.99447, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 528us/sample - loss: 0.0390 - accuracy: 0.9876 - val_loss: 0.0212 - val_accuracy: 0.9945\n",
            "Epoch 7/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0349 - accuracy: 0.9910\n",
            "Epoch 00007: val_accuracy improved from 0.99447 to 0.99642, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 525us/sample - loss: 0.0348 - accuracy: 0.9910 - val_loss: 0.0155 - val_accuracy: 0.9964\n",
            "Epoch 8/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9946\n",
            "Epoch 00008: val_accuracy did not improve from 0.99642\n",
            "4099/4099 [==============================] - 2s 516us/sample - loss: 0.0191 - accuracy: 0.9946 - val_loss: 0.0218 - val_accuracy: 0.9928\n",
            "Epoch 9/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9975\n",
            "Epoch 00009: val_accuracy improved from 0.99642 to 0.99902, saving model to best-model.hdf5\n",
            "4099/4099 [==============================] - 2s 524us/sample - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0044 - val_accuracy: 0.9990\n",
            "Epoch 10/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9975\n",
            "Epoch 00010: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0116 - val_accuracy: 0.9964\n",
            "Epoch 11/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9936\n",
            "Epoch 00011: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.0222 - val_accuracy: 0.9964\n",
            "Epoch 12/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9948\n",
            "Epoch 00012: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0262 - accuracy: 0.9946 - val_loss: 0.0174 - val_accuracy: 0.9938\n",
            "Epoch 13/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9945\n",
            "Epoch 00013: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0198 - accuracy: 0.9944 - val_loss: 0.0170 - val_accuracy: 0.9948\n",
            "Epoch 14/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9965\n",
            "Epoch 00014: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0116 - val_accuracy: 0.9961\n",
            "Epoch 15/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9978\n",
            "Epoch 00015: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 508us/sample - loss: 0.0053 - accuracy: 0.9978 - val_loss: 0.0225 - val_accuracy: 0.9935\n",
            "Epoch 16/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9950\n",
            "Epoch 00016: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0283 - accuracy: 0.9944 - val_loss: 0.0125 - val_accuracy: 0.9958\n",
            "Epoch 17/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9987\n",
            "Epoch 00017: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.0239 - val_accuracy: 0.9945\n",
            "Epoch 18/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9965\n",
            "Epoch 00018: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0142 - accuracy: 0.9966 - val_loss: 0.0193 - val_accuracy: 0.9964\n",
            "Epoch 19/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9946\n",
            "Epoch 00019: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0119 - val_accuracy: 0.9967\n",
            "Epoch 20/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988\n",
            "Epoch 00020: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0169 - val_accuracy: 0.9958\n",
            "Epoch 21/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9948\n",
            "Epoch 00021: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 0.0250 - accuracy: 0.9946 - val_loss: 0.0191 - val_accuracy: 0.9935\n",
            "Epoch 22/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9942\n",
            "Epoch 00022: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0213 - accuracy: 0.9944 - val_loss: 0.0320 - val_accuracy: 0.9945\n",
            "Epoch 23/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9940\n",
            "Epoch 00023: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 0.0213 - accuracy: 0.9941 - val_loss: 0.0259 - val_accuracy: 0.9932\n",
            "Epoch 24/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9983\n",
            "Epoch 00024: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 0.0102 - accuracy: 0.9983 - val_loss: 0.0139 - val_accuracy: 0.9958\n",
            "Epoch 25/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9988\n",
            "Epoch 00025: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0157 - val_accuracy: 0.9938\n",
            "Epoch 26/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990\n",
            "Epoch 00026: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 507us/sample - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0081 - val_accuracy: 0.9977\n",
            "Epoch 27/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n",
            "Epoch 00027: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.0163 - val_accuracy: 0.9951\n",
            "Epoch 28/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9985\n",
            "Epoch 00028: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0090 - val_accuracy: 0.9974\n",
            "Epoch 29/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 5.2966e-04 - accuracy: 1.0000\n",
            "Epoch 00029: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 6.2120e-04 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 0.9980\n",
            "Epoch 30/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 4.7190e-04 - accuracy: 1.0000\n",
            "Epoch 00030: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 4.7477e-04 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 0.9980\n",
            "Epoch 31/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
            "Epoch 00031: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 516us/sample - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0121 - val_accuracy: 0.9971\n",
            "Epoch 32/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 9.4327e-04 - accuracy: 0.9995\n",
            "Epoch 00032: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 9.4258e-04 - accuracy: 0.9995 - val_loss: 0.0097 - val_accuracy: 0.9977\n",
            "Epoch 33/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9990\n",
            "Epoch 00033: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0016 - accuracy: 0.9990 - val_loss: 0.0450 - val_accuracy: 0.9958\n",
            "Epoch 34/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9958\n",
            "Epoch 00034: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.0437 - val_accuracy: 0.9893\n",
            "Epoch 35/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9893\n",
            "Epoch 00035: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0402 - accuracy: 0.9893 - val_loss: 0.0709 - val_accuracy: 0.9880\n",
            "Epoch 36/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9932\n",
            "Epoch 00036: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0451 - accuracy: 0.9932 - val_loss: 0.0170 - val_accuracy: 0.9961\n",
            "Epoch 37/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9985\n",
            "Epoch 00037: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 507us/sample - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0173 - val_accuracy: 0.9961\n",
            "Epoch 38/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9988\n",
            "Epoch 00038: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0159 - val_accuracy: 0.9971\n",
            "Epoch 39/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 00039: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0263 - val_accuracy: 0.9961\n",
            "Epoch 40/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9988\n",
            "Epoch 00040: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0129 - val_accuracy: 0.9974\n",
            "Epoch 41/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
            "Epoch 00041: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0161 - val_accuracy: 0.9974\n",
            "Epoch 42/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
            "Epoch 00042: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0294 - val_accuracy: 0.9938\n",
            "Epoch 43/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 00043: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.0091 - val_accuracy: 0.9980\n",
            "Epoch 44/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9988\n",
            "Epoch 00044: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0090 - val_accuracy: 0.9980\n",
            "Epoch 45/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 5.4093e-04 - accuracy: 1.0000\n",
            "Epoch 00045: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 5.2491e-04 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9984\n",
            "Epoch 46/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9995\n",
            "Epoch 00046: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.0041 - val_accuracy: 0.9990\n",
            "Epoch 47/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9985\n",
            "Epoch 00047: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.0084 - val_accuracy: 0.9974\n",
            "Epoch 48/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
            "Epoch 00048: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
            "Epoch 49/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 7.4666e-04 - accuracy: 0.9998\n",
            "Epoch 00049: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 7.3451e-04 - accuracy: 0.9998 - val_loss: 0.0046 - val_accuracy: 0.9974\n",
            "Epoch 50/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 7.3599e-04 - accuracy: 0.9998\n",
            "Epoch 00050: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 7.2406e-04 - accuracy: 0.9998 - val_loss: 0.0056 - val_accuracy: 0.9977\n",
            "Epoch 51/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 5.0938e-04 - accuracy: 0.9998\n",
            "Epoch 00051: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 5.0901e-04 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9984\n",
            "Epoch 52/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 2.9854e-04 - accuracy: 0.9998\n",
            "Epoch 00052: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 2.9832e-04 - accuracy: 0.9998 - val_loss: 0.0073 - val_accuracy: 0.9980\n",
            "Epoch 53/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9962\n",
            "Epoch 00053: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 508us/sample - loss: 0.0281 - accuracy: 0.9961 - val_loss: 0.2241 - val_accuracy: 0.9691\n",
            "Epoch 54/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9914\n",
            "Epoch 00054: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0429 - accuracy: 0.9912 - val_loss: 0.0227 - val_accuracy: 0.9945\n",
            "Epoch 55/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9972\n",
            "Epoch 00055: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0178 - accuracy: 0.9971 - val_loss: 0.0216 - val_accuracy: 0.9951\n",
            "Epoch 56/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 00056: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0165 - val_accuracy: 0.9971\n",
            "Epoch 57/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9980\n",
            "Epoch 00057: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0134 - accuracy: 0.9980 - val_loss: 0.0115 - val_accuracy: 0.9980\n",
            "Epoch 58/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9988\n",
            "Epoch 00058: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.0419 - val_accuracy: 0.9954\n",
            "Epoch 59/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9987\n",
            "Epoch 00059: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0116 - val_accuracy: 0.9967\n",
            "Epoch 60/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9978\n",
            "Epoch 00060: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0115 - accuracy: 0.9978 - val_loss: 0.0197 - val_accuracy: 0.9948\n",
            "Epoch 61/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9975\n",
            "Epoch 00061: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.0162 - val_accuracy: 0.9961\n",
            "Epoch 62/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9968\n",
            "Epoch 00062: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0106 - val_accuracy: 0.9974\n",
            "Epoch 63/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 00063: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9974\n",
            "Epoch 64/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9973\n",
            "Epoch 00064: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0110 - val_accuracy: 0.9980\n",
            "Epoch 65/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9980\n",
            "Epoch 00065: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.0128 - val_accuracy: 0.9977\n",
            "Epoch 66/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
            "Epoch 00066: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0131 - val_accuracy: 0.9974\n",
            "Epoch 67/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9992\n",
            "Epoch 00067: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0181 - val_accuracy: 0.9977\n",
            "Epoch 68/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9992\n",
            "Epoch 00068: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0072 - accuracy: 0.9993 - val_loss: 0.0094 - val_accuracy: 0.9984\n",
            "Epoch 69/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9995\n",
            "Epoch 00069: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0049 - accuracy: 0.9995 - val_loss: 0.0087 - val_accuracy: 0.9980\n",
            "Epoch 70/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 3.4182e-04 - accuracy: 1.0000\n",
            "Epoch 00070: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 3.4699e-04 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9980\n",
            "Epoch 71/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
            "Epoch 00071: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0093 - val_accuracy: 0.9984\n",
            "Epoch 72/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 8.5761e-04 - accuracy: 0.9998\n",
            "Epoch 00072: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 8.5698e-04 - accuracy: 0.9998 - val_loss: 0.0101 - val_accuracy: 0.9980\n",
            "Epoch 73/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 6.3447e-04 - accuracy: 0.9997\n",
            "Epoch 00073: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 506us/sample - loss: 6.1455e-04 - accuracy: 0.9998 - val_loss: 0.0118 - val_accuracy: 0.9974\n",
            "Epoch 74/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 9.2390e-04 - accuracy: 0.9998\n",
            "Epoch 00074: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 9.0889e-04 - accuracy: 0.9998 - val_loss: 0.0161 - val_accuracy: 0.9980\n",
            "Epoch 75/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
            "Epoch 00075: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0114 - val_accuracy: 0.9974\n",
            "Epoch 76/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 5.3703e-05 - accuracy: 1.0000\n",
            "Epoch 00076: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 5.2945e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9974\n",
            "Epoch 77/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 6.2964e-05 - accuracy: 1.0000\n",
            "Epoch 00077: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 6.1002e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9974\n",
            "Epoch 78/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 3.0027e-05 - accuracy: 1.0000\n",
            "Epoch 00078: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 3.0005e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9974\n",
            "Epoch 79/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 3.8340e-05 - accuracy: 1.0000\n",
            "Epoch 00079: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 3.8313e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9974\n",
            "Epoch 80/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 5.9940e-05 - accuracy: 1.0000\n",
            "Epoch 00080: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 6.1903e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9977\n",
            "Epoch 81/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 6.9443e-05 - accuracy: 1.0000\n",
            "Epoch 00081: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 6.9392e-05 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 0.9974\n",
            "Epoch 82/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 7.2022e-04 - accuracy: 0.9998\n",
            "Epoch 00082: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 515us/sample - loss: 7.1969e-04 - accuracy: 0.9998 - val_loss: 0.0105 - val_accuracy: 0.9977\n",
            "Epoch 83/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 2.5712e-05 - accuracy: 1.0000\n",
            "Epoch 00083: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 2.5693e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9980\n",
            "Epoch 84/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 7.4964e-04 - accuracy: 0.9995\n",
            "Epoch 00084: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 7.4910e-04 - accuracy: 0.9995 - val_loss: 0.0121 - val_accuracy: 0.9980\n",
            "Epoch 85/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 1.5944e-04 - accuracy: 1.0000\n",
            "Epoch 00085: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 1.5932e-04 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9980\n",
            "Epoch 86/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 5.5193e-05 - accuracy: 1.0000\n",
            "Epoch 00086: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 5.5758e-05 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9980\n",
            "Epoch 87/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 2.8397e-05 - accuracy: 1.0000\n",
            "Epoch 00087: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 511us/sample - loss: 2.8467e-05 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 0.9980\n",
            "Epoch 88/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 3.3706e-04 - accuracy: 0.9998\n",
            "Epoch 00088: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 505us/sample - loss: 3.3681e-04 - accuracy: 0.9998 - val_loss: 0.0195 - val_accuracy: 0.9980\n",
            "Epoch 89/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9995\n",
            "Epoch 00089: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 508us/sample - loss: 0.0070 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9977\n",
            "Epoch 90/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
            "Epoch 00090: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0087 - val_accuracy: 0.9984\n",
            "Epoch 91/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9988\n",
            "Epoch 00091: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0131 - val_accuracy: 0.9964\n",
            "Epoch 92/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 7.5063e-04 - accuracy: 0.9997\n",
            "Epoch 00092: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 508us/sample - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0184 - val_accuracy: 0.9971\n",
            "Epoch 93/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0242 - accuracy: 0.9955\n",
            "Epoch 00093: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.0239 - accuracy: 0.9951 - val_loss: 0.0492 - val_accuracy: 0.9902\n",
            "Epoch 94/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.2752 - accuracy: 0.9794\n",
            "Epoch 00094: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 508us/sample - loss: 0.2755 - accuracy: 0.9788 - val_loss: 0.2449 - val_accuracy: 0.9743\n",
            "Epoch 95/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9878\n",
            "Epoch 00095: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 509us/sample - loss: 0.1061 - accuracy: 0.9878 - val_loss: 0.0648 - val_accuracy: 0.9938\n",
            "Epoch 96/100\n",
            "4032/4099 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9970\n",
            "Epoch 00096: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 507us/sample - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0401 - val_accuracy: 0.9964\n",
            "Epoch 97/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9963\n",
            "Epoch 00097: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 512us/sample - loss: 0.0238 - accuracy: 0.9963 - val_loss: 0.0644 - val_accuracy: 0.9935\n",
            "Epoch 98/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9977\n",
            "Epoch 00098: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 513us/sample - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.0413 - val_accuracy: 0.9964\n",
            "Epoch 99/100\n",
            "4096/4099 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9988\n",
            "Epoch 00099: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 514us/sample - loss: 0.0100 - accuracy: 0.9988 - val_loss: 0.0130 - val_accuracy: 0.9987\n",
            "Epoch 100/100\n",
            "3968/4099 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9985\n",
            "Epoch 00100: val_accuracy did not improve from 0.99902\n",
            "4099/4099 [==============================] - 2s 510us/sample - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0197 - val_accuracy: 0.9974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TLiHB2yWiG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"best-model.hdf5\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1lFalcrZA4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "bc014480-1f1b-4a91-94cc-6a9177e4568e"
      },
      "source": [
        "Y_pred_test = model.predict(X_test)\n",
        "y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
        "\n",
        "classification = classification_report(np.argmax(y_test, axis=1), y_pred_test, digits=6)\n",
        "print(classification)"
      ],
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   1.000000  1.000000  1.000000        14\n",
            "           1   1.000000  0.997664  0.998830       428\n",
            "           2   0.988095  1.000000  0.994012       249\n",
            "           3   1.000000  1.000000  1.000000        71\n",
            "           4   1.000000  0.986207  0.993056       145\n",
            "           5   1.000000  1.000000  1.000000       219\n",
            "           6   1.000000  1.000000  1.000000         8\n",
            "           7   1.000000  1.000000  1.000000       144\n",
            "           8   1.000000  1.000000  1.000000         6\n",
            "           9   1.000000  1.000000  1.000000       292\n",
            "          10   0.997294  1.000000  0.998645       737\n",
            "          11   1.000000  0.988764  0.994350       178\n",
            "          12   1.000000  1.000000  1.000000        61\n",
            "          13   1.000000  1.000000  1.000000       379\n",
            "          14   1.000000  0.991379  0.995671       116\n",
            "          15   0.965517  1.000000  0.982456        28\n",
            "\n",
            "    accuracy                       0.998049      3075\n",
            "   macro avg   0.996932  0.997751  0.997314      3075\n",
            "weighted avg   0.998073  0.998049  0.998050      3075\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxI-mlwIZM__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsM1VuDETOff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reports (X_test,y_test,name):\n",
        "    #start = time.time()\n",
        "    Y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    #end = time.time()\n",
        "    #print(end - start)\n",
        "    if name == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    elif name == 'SA':\n",
        "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
        "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
        "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
        "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
        "    elif name == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
        "                        'Self-Blocking Bricks','Shadows']\n",
        "    \n",
        "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    \n",
        "    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkRcovU5TRMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "771bd9b6-c412-4374-b965-89cf2f79ae92"
      },
      "source": [
        "classification, confusion, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(X_test,y_test,dataset)\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "file_name = \"classification_report.txt\"\n",
        "\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write('{} Test loss (%)'.format(Test_loss))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(classification))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(confusion))"
      ],
      "execution_count": 491,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3075/3075 [==============================] - 1s 248us/sample - loss: 0.0066 - accuracy: 0.9980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRpzkK_7TUaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    \n",
        "    return patch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXpAyLP1TyeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the original image\n",
        "X, y = loadData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzBQjutPT0Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height = y.shape[0]\n",
        "width = y.shape[1]\n",
        "PATCH_SIZE = windowSize\n",
        "numComponents = K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqUqi_sJT1Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,pca = applyPCA(X, numComponents=numComponents)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSIHlVDlT6XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = padWithZeros(X, PATCH_SIZE//2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eJ7ILkGT75u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the predicted image\n",
        "outputs = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(y[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(X,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')                                   \n",
        "            prediction = (model.predict(X_test_image))\n",
        "            prediction = np.argmax(prediction, axis=1)\n",
        "            outputs[i][j] = prediction+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK5POKuRT-Ah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "faaf2689-5863-41b7-f124-6e9fffe8a3ab"
      },
      "source": [
        "ground_truth = spectral.imshow(classes = y,figsize =(7,7))"
      ],
      "execution_count": 500,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAGeCAYAAADbrXX+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRQzrnWA+bwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakDDBzWU20j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "cd217b50-9676-46d6-9ef3-a7e751fb950f"
      },
      "source": [
        "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(7,7))"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAGeCAYAAADbrXX+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJJUEASEZEkKCCJiEgS/n/+49xinopNuwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHlSQ6ZCU6Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spectral.save_rgb(\"predictions.jpg\", outputs.astype(int), colors=spectral.spy_colors)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}